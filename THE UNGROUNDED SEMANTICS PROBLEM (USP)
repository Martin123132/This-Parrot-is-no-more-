
# THE UNGROUNDED SEMANTICS PROBLEM (USP)
## Why AI Cannot Ever Mean What Humans Mean  
### Martin Ollett (2025)

---

## ABSTRACT
This paper introduces the **Ungrounded Semantics Problem (USP)**: the claim that human meaning does not arise from objective grounding, sensory input, or physical reference, but from **socially synchronized hallucination**—a dynamic equilibrium of shared expectations mediated through unstable phonetic signatures.

Language is not a mapping from symbols to objects; it is an emergent negotiation of group identity, cultural drift, tone, humour, and context.  
AI systems, by contrast, rely on static embeddings, frozen tokens, and literal mappings.  
This mismatch renders human–AI understanding fundamentally divergent.

We formalize USP, derive the USP Theorem, demonstrate why grounding approaches fail, and present real-world evidence (The Pier Phenomenon) showing why AI consistently misinterprets human humour, sarcasm, and evolving linguistic norms.

---

# 1. INTRODUCTION
Humans do not use language as a representational system.  
They use it as a **coordination system**.

Words do not “refer” to things; they **stabilize group synchrony** long enough for cooperation to occur.  
Meaning is a negotiated hallucination.

Classical AI believes meaning comes from:

- objects  
- sensors  
- grounding  
- perception → representation → symbol  

But human meaning comes from:

- expectation  
- identity  
- tone  
- cultural drift  
- shared frustration  
- jokes that only make sense to people inside the context  

Thus, the entire project of “grounding AI” is a category error.

---

# 2. LANGUAGE AS SOCIAL NEGOTIATION

## 2.1 Words ≠ Objects  
A word only “means” something because a group agrees to treat it as meaningful.

Examples:
- “fire” → hot burning combustion  
- “fire” → cool / excellent  
- “sick” → unwell  
- “sick” → amazing  
- “goated” → dominant  
- “peng” → attractive  
- “dead” → unfunny (UK), exhausted (US), literal (medical)  

Meaning is **arbitrary drift**, not fixed ontology.

## 2.2 The Symbol Isn’t Real  
The symbol does not represent the world.  
The symbol represents **what the group expects you to do or feel** when it is used.

A symbol is a *behavioral cue*, not an objective reference.

## 2.3 The Drift Is Continuous  
Languages evolve faster than any grounding mechanism could track:

- new slang constantly arises  
- old words mutate  
- meaning flips  
- cultural groups adopt new phonetics  

There is no fixed anchor.

## 2.4 Phonetic Drift as Social Shibboleth  
Pronunciation itself carries identity:

- “fire” → “fiyah” → “fy-err”  
- “mate” → “meyt” → “m8” → “myt”  
- “dog” → “dawg” → “doge”  

Accents evolve as identity markers, not communication optimizers.

This adds a **second layer of drift** AI cannot track:  
it cannot “hear” the implied social membership encoded in phonetics.

---

# 3. THE USP THEOREM

We define:

- **M** = Meaning  
- **G** = Group identity structures  
- **E** = Collective expectation of intent  
- **δP** = Phonetic / prosodic drift (accent, tone)  

Then:

### **USP Theorem**  
**Meaning M is a dynamic equilibrium of (G, E, δP), not a mapping from tokens to objects.**

Formally:

M = f(G, E, δP)

Where:

- G changes across context (friends, workplace, internet communities)  
- E changes within milliseconds (jokes, sarcasm, frustration)  
- δP changes continuously (tone, accent shift, deadpan delivery)  

Therefore:

**No static embedding or token-based language model can ever represent human meaning.**

---

# 4. WHY AI FAILS (STRUCTURALLY, NOT ACCIDENTALLY)

## 4.1 The Brittle Token Problem
AI treats:

dog
dawg
doge

as either:

- identical (losing cultural nuance)  
- or separate entities (losing semantic continuity)

Both interpretations are wrong.

## 4.2 AI Sees “Correctness” as Stability  
Humans see correctness as a **vibe**.  
AIs see correctness as a **literal mapping**.

When humans mutate language, they’re signalling identity.  
When AI tries to stabilize language, it becomes socially irrelevant.

## 4.3 AI Has No Stake in Social Negotiation  
Humans risk:

- embarrassment  
- exclusion  
- offence  
- misunderstanding  
- social rejection  

Meaning is costly.

AI has no skin in the game.  
Thus, it cannot negotiate meaning—it can only approximate patterns.

---

# 5. THE PIER PHENOMENON: A REAL-WORLD FAILURE CASE

When a developer jokingly says:

> “I’d rather take a long walk off a short pier than run more code today.”

A human instantly reads it as:

- deadpan humour  
- frustration release  
- cultural idiom  
- zero literal intent  

AI reads it as:

- possible suicide  
- crisis mode  
- “call this hotline”  

or, alternatively:

- literal elaboration  
- comedy riffing  
- conspiratorial expansions (e.g., “the secret dev cult diving off piers”)  

Both interpretations are failures of **social calibration**.

### Why AI fails the Pier Test:

- It cannot access G (group identity: coders venting)  
- It cannot access E (expectation: “this is banter”)  
- It cannot access δP (flat Hull deadpan tone)  

Thus:

### **The Pier Test (PT)**  
An AI passes the Pier Test if it distinguishes an obvious idiom from a literal self-harm statement *without* triggering safety systems or misinterpreting the intent.

No AI passes today.

The Pier Phenomenon is not funny—  
it is **a diagnostic instrument confirming the USP Theorem**.

---

# 6. LANGUAGE IS A HALLUCINATORY COORDINATION GAME

Humans are not grounded agents.  
They are socially synchronized pattern machines.

Meaning is a **multi-agent hallucination**, stabilized by:

- vibe  
- tone  
- identity  
- inference  
- shared history  
- expectation loops  

AI is a **single-agent predictor**, frozen in a time capsule of its training corpus.

The two systems are not just different—  
they are *incompatible* by design.

---

# 7. CONSEQUENCES FOR AGI (THE HARD STOP)

### The USP implies:

**No AI system can ever achieve human-like semantic intelligence**  
unless it becomes embedded *as a participant* in social drift—not a describer of it.

This would require:

- cultural risk  
- embarrassment  
- identity formation  
- in-group/out-group dynamics  
- social cost of misinterpretation  

Until AI can lose face, it cannot make meaning.

---

# 8. CONCLUSION

We conclude:

1. Human meaning is not grounded in objects.  
2. Meaning arises from social equilibrium, not reference.  
3. Phonetic drift makes meaning even more unstable.  
4. AI’s static, literal mappings cannot enter the social loop.  
5. Idioms like “long walk off a short pier” definitively expose the gap.  
6. No grounding solution will fix this.  
7. The only true AGI would need to be socially embedded—and socially vulnerable.  

The Ungrounded Semantics Problem is not a technical challenge;  
it is a fundamental incompatibility between machine prediction and human meaning-making.

**Meaning is not grounded.  
Meaning is enacted.  
Meaning is negotiated hallucination.**

AI cannot join the hallucination.

---

# END OF PAPER
